{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# James Camagong's DJ Set Analysis - Data Scraping and Spotify API Call\n",
    "\n",
    "This Notebook explains how I scraped DJ set data from www.1001tracklists.com and passed those songs through the Spotify API to retrieve the audio feautures for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the data using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the DJs I chose for analysis and their respective 1001tracklists page which lists all the tracklists\n",
    "# that are available to view. We will scrape the tracklist URLs from these later.\n",
    "\n",
    "dj_pages = [\n",
    "       ['https://www.1001tracklists.com/dj/alesso/index.html', 'alesso'],\n",
    "       ['https://www.1001tracklists.com/dj/porterrobinson/index.html', 'porter_robinson'],\n",
    "       ['https://www.1001tracklists.com/dj/kaskade/index.html', 'kaskade'],\n",
    "       ['https://www.1001tracklists.com/dj/skrillex/index.html', 'skrillex'],\n",
    "       ['https://www.1001tracklists.com/dj/diplo/index.html', 'diplo'],\n",
    "       ['https://www.1001tracklists.com/dj/martingarrix/index.html', 'martin_garrix'],\n",
    "       ['https://www.1001tracklists.com/dj/zedd/index.html', 'zedd'],\n",
    "       ['https://www.1001tracklists.com/dj/djsnake/index.html', 'dj_snake'],\n",
    "       ['https://www.1001tracklists.com/dj/illenium/index.html', 'illenium'],\n",
    "       ['https://www.1001tracklists.com/dj/deadmau5/index.html', 'deadmau5']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "Websites usually have security to prevent too many requests to their server for DDoS attacks at the like. Scraping the data was kind of a struggle because www.1001tracklists.com has pretty good security and I was only allowed a few requests before getting blocked. \n",
    "\n",
    "Here I fake a user agent and in later cells I use a free proxy service to cycle IPs. This code might not work anymore and depends on the quality of the proxy IPs provided and the security of the site. I thank www.1001tracklists.com for the data and I never wanted to be a strain on their server. Make sure to not spam too many requests at once and to have a wait time between requests so you do not cause problems with your web scraping :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import random\n",
    "from random import uniform\n",
    "import shadow_useragent\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# Scrapes the individual URLs for the individual tracklists from the artist pages I hardcoded above \n",
    "ua = shadow_useragent.ShadowUserAgent()\n",
    "hdr = {\"User-Agent\":ua.random_nomobile}\n",
    "\n",
    "dj_tracklists = []\n",
    "\n",
    "for link in dj_pages:\n",
    "  sleep(random.uniform(1, 3))\n",
    "  page_open = requests.get(link[0], headers = hdr).text\n",
    "  soup = BeautifulSoup(page_open)\n",
    "  \n",
    "  tracklists = []\n",
    "  for a in soup.find_all('a', href=True):\n",
    "    if '/tracklist/' in str(a['href']):\n",
    "      tracklists.append('https://www.1001tracklists.com' + str(a['href']))\n",
    "\n",
    "  dj_tracklists.append([link[1], tracklists[:9]])\n",
    "\n",
    "print(dj_tracklists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "from fake_useragent import UserAgent\n",
    "from IPython.core.display import clear_output\n",
    "\n",
    "\n",
    "# This function scrapes a free proxy provider website for IPs to use while making requests. It returns a \n",
    "# list of IP addresses which will be used later.\n",
    "# Thanks to the contributors here for this code:\n",
    "# https://stackoverflow.com/questions/38785877/spoofing-ip-address-when-web-scraping-python?noredirect=1&lq=1\n",
    "\n",
    "def get_proxies():\n",
    "  # Here I provide some proxies for not getting caught while scraping\n",
    "  ua = UserAgent() # From here we generate a random user agent\n",
    "  proxies = [] # Will contain proxies [ip, port]\n",
    "\n",
    "  # Retrieve latest proxies\n",
    "  headers= {'User-Agent': ua.random, \"Accept-Language\": \"en-US, en;q=0.5\"}\n",
    "  urlopen = requests.get('https://www.sslproxies.org/', headers = headers).text\n",
    "  soup = BeautifulSoup(urlopen, 'lxml')\n",
    "  proxies_table = soup.find(id='proxylisttable')\n",
    "\n",
    "  # Save proxies in the array\n",
    "  for row in proxies_table.tbody.find_all('tr'):\n",
    "    proxies.append({\n",
    "      'ip':   row.find_all('td')[0].string,\n",
    "      'port': row.find_all('td')[1].string\n",
    "    })\n",
    "\n",
    "  # Choose a random proxy\n",
    "  proxy_index = random.randint(0, len(proxies) - 1)\n",
    "  proxy = proxies[proxy_index]\n",
    "\n",
    "  for n in range(1, 20):\n",
    "    req = Request('http://icanhazip.com')\n",
    "    req.set_proxy(proxy['ip'] + ':' + proxy['port'], 'http')\n",
    "\n",
    "    # Every 10 requests, generate a new proxy\n",
    "    if n % 10 == 0:\n",
    "      proxy_index = random.randint(0, len(proxies) - 1)\n",
    "      proxy = proxies[proxy_index]\n",
    "\n",
    "    # Make the call\n",
    "    try:\n",
    "      my_ip = urlopen(req).read().decode('utf8')\n",
    "      print('#' + str(n) + ': ' + my_ip)\n",
    "      clear_output(wait = True)\n",
    "    except: # If error, delete this proxy and find another one\n",
    "      del proxies[proxy_index]\n",
    "      print('Proxy ' + proxy['ip'] + ':' + proxy['port'] + ' deleted.')\n",
    "      proxy_index = random.randint(0, len(proxies) - 1)\n",
    "      proxy = proxies[proxy_index]\n",
    "\n",
    "  global converted_proxies \n",
    "  converted_proxies = []\n",
    "  for i in proxies:\n",
    "    k = {'https':'https://'+str(i['ip']+':'+str(i['port']))}\n",
    "    converted_proxies.append(k)\n",
    "  \n",
    "  return converted_proxies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "from requests.exceptions import ProxyError\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "\n",
    "# This code retrieves the individual webpage for each track on 1001tracklists.com. These links are needed because \n",
    "# the body of the webpage has the unique Spotify IDs of each track which makes it easy to get the music features\n",
    "# from the Spotify API.\n",
    "\n",
    "ua = UserAgent() \n",
    "\n",
    "retry_strategy = Retry(\n",
    "    total=5,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "http = requests.Session()\n",
    "http.mount(\"https://\", adapter)\n",
    "http.mount(\"http://\", adapter)\n",
    "\n",
    "tracks_from_tracklists = []\n",
    "track_urls = []\n",
    "\n",
    "for i in dj_tracklists:\n",
    "    for k in i[1]:\n",
    "        exception = True\n",
    "        while (exception):\n",
    "            exception = False\n",
    "      try:\n",
    "          sleep(random.uniform(1, 3))\n",
    "          headers= {'User-Agent': ua.random, \"Accept-Language\": \"en-US, en;q=0.5\"}\n",
    "          proxy = random.choice(converted_proxies)\n",
    "          if len(converted_proxies) > 0:\n",
    "            page_open = http.get(k, headers=headers, proxies=proxy).text\n",
    "          else:\n",
    "            page_open = http.get(k, headers=headers).text\n",
    "          soup = BeautifulSoup(page_open)\n",
    "\n",
    "          meta_tag = soup.find_all('meta', attrs={'itemprop': 'url'})\n",
    "          counter = 0\n",
    "          for each in meta_tag:\n",
    "            if \"/track/\" in str(each):\n",
    "              url = 'https://www.1001tracklists.com' + str(each).split('\"')[1]\n",
    "              counter += 1\n",
    "              x = {'id':counter, 'tracklist_link':k, 'track_link':url, 'artist':str(i[0])}\n",
    "              print(x)\n",
    "              track_urls.append(x)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "        print(proxy)\n",
    "        converted_proxies.remove(proxy)\n",
    "        exception = True\n",
    "\n",
    "df = pd.DataFrame(track_urls)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.exceptions import ProxyError\n",
    "\n",
    "\n",
    "# This block of code is the largest web scraping needed for this project, ~1000 requests. We take the individual \n",
    "# track URLs on 1001tracklists and find the Spotify ID for each track so that we can later pass it through the \n",
    "# Spotify API. We do this in chunks because the free Proxy IPs get banned after ~20 requests. After a list of IPs\n",
    "# is exhausted, the IP retrieval function gets new list of IPs until we have made the 1000 requests.\n",
    "\n",
    "ua = UserAgent() # From here we generate a random user agent\n",
    "\n",
    "spotify_ids = []\n",
    "\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "    # Thank you to the contributors here for this useful function that allows you to iterate over a list in chunks\n",
    "    # https://stackoverflow.com/questions/434287/what-is-the-most-pythonic-way-to-iterate-over-a-list-in-chunks\n",
    "\n",
    "counter = 0\n",
    "for chunk in chunker(track_links,20):\n",
    "    get_proxies()\n",
    "    proxy_pool = converted_proxies\n",
    "    print(proxy_pool)\n",
    "    for url in chunk:\n",
    "    exception = True\n",
    "    while (exception):\n",
    "        exception = False\n",
    "        try:\n",
    "        headers= {'User-Agent': ua.random, \"Accept-Language\": \"en-US, en;q=0.5\"}\n",
    "        sleep(random.uniform(1, 3))\n",
    "        proxy = random.choice(proxy_pool)\n",
    "        urlopen = http.get(url, headers = headers, proxies=proxy).text\n",
    "\n",
    "        soup2 = BeautifulSoup(urlopen, 'lxml')\n",
    "\n",
    "        classes = []\n",
    "        for element in soup2.find_all(class_= True):\n",
    "            classes.extend(element[\"class\"])\n",
    "\n",
    "        for cl in classes:\n",
    "            if 'mediaItem' in str(cl) and len(str(cl))==31:\n",
    "                j = {'track_link':url, 'spotify_id':cl.split('Item')[1]}\n",
    "                spotify_ids.append(j) \n",
    "                counter += 1\n",
    "                print(counter, j)\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            proxy_pool.remove(proxy)\n",
    "            exception = True\n",
    "\n",
    "print(spotify_ids)\n",
    "\n",
    "# And now we're finally done scraping data off 1001tracklists :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving data from the Spotify API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy \n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import requests\n",
    "\n",
    "# Spotipy is a great package that simplifies using the Spotify API. Here we initialize the session. You will \n",
    "# need to follow the Spotify instructions on how to get a client ID and client secret on their webpage.\n",
    "\n",
    "cid = 'insert here'\n",
    "secret = 'insert here'\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id=cid, client_secret=secret, requests_timeout=100)\n",
    "sp = spotipy.Spotify(client_credentials_manager = client_credentials_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This is a simple loop to retrieve the audio features which are in dictionary form.\n",
    "\n",
    "audio_feature_dicts = []\n",
    "\n",
    "for chunk in chunker(spotify_ids,99):\n",
    "  sleep(random.uniform(1, 3))\n",
    "  audio_json = sp.audio_features(chunk)\n",
    "  for each in audio_json:\n",
    "    audio_feature_dicts.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# And here we construct a Pandas dataframe that is ready for analysis\n",
    "df_features = pd.DataFrame(audio_feature_dicts)\n",
    "df_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
